{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================\n# FULL COLAB NOTEBOOK: LLaVA-1.5 7B (HF) + SPIN + MMHal-Bench + Qwen2.5 Judge\n# ============================================\n# ‚úÖ Fixes applied vs your version:\n# (1) SPIN head scoring uses ATTENTION PROBABILITIES (softmax), not raw logits\n# (2) SPIN hyperparams match paper guidance: r=0.05 keep 95% heads (LLaVA-7B)\n# (3) SPIN applies only on decode stage (q_len == 1), preserving prefill grounding\n# (4) Image token indices are discovered robustly via <image> placeholder token positions\n# (5) Debug counters verify SPIN is actually active during generation\n# (6) Evaluation loop outputs mmhal judge JSON + score stats\n#\n# Paper reference: \"Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression\" (SPIN)\n# see Eq. (3)-(4) and ablation Table 6 for LLaVA-1.5 7B best r=0.05. \n\n\n# =====================================================\n# 0) INSTALLS\n# =====================================================\n# !pip install -q -U numpy==1.26.4\n# !pip install -q torch==2.2.2+cu118 torchvision==0.17.2+cu118 torchaudio==2.2.2+cu118 --index-url https://download.pytorch.org/whl/cu118\n# !pip install -q transformers==4.37.0 accelerate==0.26.1 bitsandbytes==0.41.1 datasets pillow tqdm\n\n# print(\"‚úÖ Libraries installed.\")","metadata":{"_uuid":"1920e52d-9a94-4b16-971c-14ddeb0fed4a","_cell_guid":"6e60b330-2ffd-42d0-b452-9ffc7d010fa4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U transformers==4.45.2 accelerate==0.33.0 bitsandbytes==0.43.3\nprint(\"‚úÖ Libraries installed.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================\n# 1) SPIN PATCH FOR LLAVA-1.5 7B (HF)  ‚úÖ UPDATED FOR NEW TRANSFORMERS\n# =====================================================\nimport math\nimport functools\nimport types\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n\n# Debug counters\nspin_debug = {\n    \"forward_calls\": 0,\n    \"spin_active_calls\": 0,\n    \"q_len1_calls\": 0,\n    \"avg_suppressed_fraction_sum\": 0.0,\n}\n\n\ndef llama_spin_forward(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    **kwargs,   # ‚úÖ CHANGE 1: accept new kwargs like cache_position, position_embeddings\n):\n    \"\"\"\n    Transformers forward-compatible LlamaAttention forward with SPIN head suppression.\n    Minimal update: **kwargs to accept new arguments.\n    \"\"\"\n\n    spin_debug[\"forward_calls\"] += 1\n\n    bsz, q_len, _ = hidden_states.size()\n\n    # Projections\n    query_states = (\n        self.q_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    key_states = (\n        self.k_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    value_states = (\n        self.v_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n\n    # KV cache length\n    kv_seq_len = key_states.shape[-2]\n    if past_key_value is not None:\n        if self.layer_idx is None:\n            raise ValueError(\"Attention cache structure changed. Ensure attention has layer_idx.\")\n        kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n\n    # ==========================\n    # Rotary embedding (new HF compatible)\n    # ==========================\n    # ‚úÖ CHANGE 2: if position_embeddings passed by HF, use it\n    # HF newer versions may provide: position_embeddings=(cos, sin)\n    position_embeddings = kwargs.get(\"position_embeddings\", None)\n    if position_embeddings is not None:\n        cos, sin = position_embeddings\n    else:\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\n    query_states, key_states = apply_rotary_pos_emb(\n        query_states, key_states, cos, sin, position_ids\n    )\n\n    # Cache update\n    if past_key_value is not None:\n        cache_kwargs = {\"sin\": sin, \"cos\": cos}\n        key_states, value_states = past_key_value.update(\n            key_states, value_states, self.layer_idx, cache_kwargs\n        )\n\n    # Raw attention logits\n    attn_logits = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n    # Apply attention mask\n    if attention_mask is not None:\n        attn_logits = attn_logits + attention_mask\n        attn_logits = torch.maximum(\n            attn_logits,\n            attn_logits.new_full((), torch.finfo(attn_logits.dtype).min),\n        )\n\n    # Convert to attention probabilities\n    attn_probs = F.softmax(attn_logits, dim=-1, dtype=torch.float32).to(query_states.dtype)\n\n    # -------------------- SPIN --------------------\n    if getattr(self, \"use_spin_img\", False) and q_len == 1:\n        spin_debug[\"spin_active_calls\"] += 1\n        spin_debug[\"q_len1_calls\"] += 1\n\n        keep_ratio = float(self.keep_head_ratio)\n        num_keep = max(1, int(round(keep_ratio * self.num_heads)))\n\n        img_start = int(self.img_start_idx)\n        img_end = int(self.img_end_idx)\n\n        img_start = max(0, min(img_start, attn_probs.shape[-1]))\n        img_end = max(img_start, min(img_end, attn_probs.shape[-1]))\n\n        head_scores = attn_probs[:, :, -1, img_start:img_end].sum(dim=-1)  # [B, Heads]\n        _, keep_idx = torch.topk(head_scores, k=num_keep, dim=1)\n\n        mask = torch.full(\n            (bsz, self.num_heads),\n            fill_value=float(self.suppression_alpha),\n            dtype=query_states.dtype,\n            device=query_states.device,\n        )\n        mask.scatter_(1, keep_idx, 1.0)\n\n        suppressed_frac = (mask != 1.0).float().mean().item()\n        spin_debug[\"avg_suppressed_fraction_sum\"] += suppressed_frac\n\n        mask = mask.view(bsz, 1, self.num_heads)\n\n    else:\n        mask = torch.ones((bsz, q_len, self.num_heads), dtype=query_states.dtype, device=query_states.device)\n\n    # ------------------ output -------------------\n    attn_output = torch.matmul(attn_probs, value_states)\n    attn_output = attn_output.transpose(1, 2).contiguous()  # [B, Q, Heads, D]\n\n    attn_output = torch.einsum(\"bqh,bqhd->bqhd\", mask, attn_output)\n\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n    attn_output = self.o_proj(attn_output)\n\n    if not output_attentions:\n        attn_probs = None\n\n    return attn_output, attn_probs, past_key_value\n\n\ndef get_llama_layers(llava_model):\n    lm = llava_model.language_model\n    if hasattr(lm, \"model\") and hasattr(lm.model, \"layers\"):\n        return lm.model.layers\n    if hasattr(lm, \"layers\"):\n        return lm.layers\n    raise AttributeError(\"Could not locate LLaMA layers.\")\n\n\ndef apply_spin_to_llava(\n    model,\n    start_layer: int,\n    end_layer: int,\n    img_start_idx: int,\n    img_end_idx: int,\n    keep_head_ratio: float = 0.95,\n    suppression_alpha: float = 0.08,\n    use_spin_img: bool = True,\n):\n    layers = get_llama_layers(model)\n    end_layer = min(end_layer, len(layers))\n\n    for i in range(start_layer, end_layer):\n        sa = layers[i].self_attn\n\n        sa.img_start_idx = int(img_start_idx)\n        sa.img_end_idx = int(img_end_idx)\n        sa.keep_head_ratio = float(keep_head_ratio)\n        sa.suppression_alpha = float(suppression_alpha)\n        sa.use_spin_img = bool(use_spin_img)\n\n        if isinstance(sa.forward, functools.partial):\n            sa.forward = sa.forward.func\n\n        sa.forward = types.MethodType(llama_spin_forward, sa)\n\n    print(\n        f\"‚úÖ SPIN patched on layers [{start_layer}, {end_layer}). \"\n        f\"keep={keep_head_ratio}, alpha={suppression_alpha}\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:40:21.602129Z","iopub.execute_input":"2026-01-15T17:40:21.602543Z","iopub.status.idle":"2026-01-15T17:40:21.624432Z","shell.execute_reply.started":"2026-01-15T17:40:21.602515Z","shell.execute_reply":"2026-01-15T17:40:21.623627Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# =====================================================\n# 2) LOAD LLAVA-1.5 7B HF\n# =====================================================\nfrom transformers import AutoProcessor, LlavaForConditionalGeneration\nfrom PIL import Image\nimport requests\nimport torch\n\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\nmodel_revision = \"a272c74\"  # known working commit with transformers 4.37\n\nprint(\"Loading LLaVA...\")\nmodel = LlavaForConditionalGeneration.from_pretrained(\n    model_id,\n    revision=model_revision,\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n    attn_implementation=\"eager\",   # ‚úÖ ADD THIS\n)\n\nprocessor = AutoProcessor.from_pretrained(model_id, revision=model_revision)\n\nlm_device = next(model.language_model.parameters()).device\nprint(\"‚úÖ Loaded. LM device:\", lm_device)","metadata":{"_uuid":"088b1002-cceb-4da5-9d02-52981779fe0f","_cell_guid":"badaf5d8-3f77-4e50-a92f-eabaef6010b8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-15T17:40:25.597256Z","iopub.execute_input":"2026-01-15T17:40:25.597777Z","iopub.status.idle":"2026-01-15T17:42:29.342989Z","shell.execute_reply.started":"2026-01-15T17:40:25.597747Z","shell.execute_reply":"2026-01-15T17:42:29.342166Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"2026-01-15 17:40:29.864837: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768498830.330968     115 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768498830.469493     115 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768498831.561290     115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768498831.561315     115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768498831.561318     115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768498831.561321     115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Loading LLaVA...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f9a1f1bccae4d918dc47e81d21faf8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8686cc57934b4a3aa0cf22845f31789e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f9309ccb0044e878fa7fdcdb9779b4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cec891736ee743e8b3ad3299e359144f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2398545b869a4262a504daa4e69fb5a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"343577028fb242d5b7c8d4e3a2f2af6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"776f8111e8384ba3a3050d7aac567708"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54b3dd506c7c476e9cad387de6928059"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/505 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac281bfeab404f5993f8574b880db919"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46f9b18bf17346f49d7e4e0846ebc4c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fd7c0ec17e0410b9fe94a3b085966ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffb19bb6d9ad44999ddc40a8928c586a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bcefb29e33e4919beb62a9c50879640"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c6b6d65d66a48c2a68505e70861ab5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/700 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34ee75fd17fb4c58a2bf3a9b041cf968"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Loaded. LM device: cuda:0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ‚úÖ Fix processor config warning (needed for transformers >=4.46)\nif not hasattr(processor, \"patch_size\") or processor.patch_size is None:\n    processor.patch_size = model.config.vision_config.patch_size\n\nif not hasattr(processor, \"vision_feature_select_strategy\") or processor.vision_feature_select_strategy is None:\n    # llava-1.5 default\n    processor.vision_feature_select_strategy = \"default\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:42:40.447245Z","iopub.execute_input":"2026-01-15T17:42:40.447922Z","iopub.status.idle":"2026-01-15T17:42:40.452429Z","shell.execute_reply.started":"2026-01-15T17:42:40.447891Z","shell.execute_reply":"2026-01-15T17:42:40.451866Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# =====================================================\n# 3) LOAD MMHAL-BENCH DATASET (DOWNLOAD ONLY IF MISSING)\n# =====================================================\n\nimport zipfile\nimport os\nimport json\nimport requests\nfrom PIL import Image\n\nMMHAL_DIR = \"mmhal_data\"\nZIP_PATH = \"test_data.zip\"\nJSON_PATH = os.path.join(MMHAL_DIR, \"response_template.json\")\nIMG_DIR = os.path.join(MMHAL_DIR, \"images\")\n\nMMHAL_URL = \"https://huggingface.co/datasets/Shengcao1006/MMHal-Bench/resolve/main/test_data.zip\"\n\n\ndef mmhal_data_ready() -> bool:\n    \"\"\"Check if dataset files already exist.\"\"\"\n    if not os.path.exists(JSON_PATH):\n        return False\n    if not os.path.exists(IMG_DIR):\n        return False\n    # check at least some images exist\n    try:\n        img_files = [f for f in os.listdir(IMG_DIR) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".webp\"))]\n        if len(img_files) == 0:\n            return False\n    except Exception:\n        return False\n    return True\n\n\ndef download_mmhal_zip():\n    print(\"‚¨áÔ∏è Downloading MMHal-Bench zip...\")\n    r = requests.get(MMHAL_URL, stream=True)\n    r.raise_for_status()\n\n    total = int(r.headers.get(\"content-length\", 0))\n    downloaded = 0\n\n    with open(ZIP_PATH, \"wb\") as f:\n        for chunk in r.iter_content(chunk_size=1024 * 1024):\n            if chunk:\n                f.write(chunk)\n                downloaded += len(chunk)\n\n    print(f\"‚úÖ Downloaded {downloaded/1e6:.1f} MB -> {ZIP_PATH}\")\n\n\ndef extract_mmhal_zip():\n    print(\"üì¶ Extracting MMHal-Bench...\")\n    os.makedirs(MMHAL_DIR, exist_ok=True)\n    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n        z.extractall(MMHAL_DIR)\n    print(\"‚úÖ Extracted into:\", MMHAL_DIR)\n\n\ndef manual_load_mmhal_bench():\n    \"\"\"Load MMHal-Bench. Download/extract only if missing.\"\"\"\n    if not mmhal_data_ready():\n        print(\"‚ö†Ô∏è MMHal data not found locally. Preparing dataset...\")\n        if not os.path.exists(ZIP_PATH):\n            download_mmhal_zip()\n        else:\n            print(f\"‚úÖ Found existing zip: {ZIP_PATH}\")\n\n        extract_mmhal_zip()\n    else:\n        print(\"‚úÖ MMHal data already present. Skipping download.\")\n\n    # Load JSON metadata\n    with open(JSON_PATH, \"r\") as f:\n        data = json.load(f)\n\n    # Attach image objects\n    formatted = []\n    missing_imgs = 0\n\n    for item in data:\n        filename = os.path.basename(item[\"image_src\"])\n        local_img = os.path.join(IMG_DIR, filename)\n\n        try:\n            img = Image.open(local_img).convert(\"RGB\")\n            item[\"image\"] = img\n            formatted.append(item)\n        except Exception as e:\n            missing_imgs += 1\n            # keep printing minimal to avoid spam\n            if missing_imgs <= 10:\n                print(\"‚ö†Ô∏è Could not load\", filename, \"->\", e)\n\n    if missing_imgs > 0:\n        print(f\"‚ö†Ô∏è Missing/unreadable images: {missing_imgs}\")\n\n    return formatted\n\n\ndataset = manual_load_mmhal_bench()\nprint(\"‚úÖ MMHal samples loaded:\", len(dataset))\n","metadata":{"_uuid":"edeab9aa-cdd7-4d02-9f07-bc3f39fd5707","_cell_guid":"f706ba1b-0d46-41aa-b33e-e6916a0f9eee","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-15T17:42:45.274398Z","iopub.execute_input":"2026-01-15T17:42:45.275016Z","iopub.status.idle":"2026-01-15T17:42:51.056991Z","shell.execute_reply.started":"2026-01-15T17:42:45.274985Z","shell.execute_reply":"2026-01-15T17:42:51.056367Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"‚úÖ MMHal data already present. Skipping download.\n‚úÖ MMHal samples loaded: 96\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# =====================================================\n# ‚úÖ Robust image token range (v4.46+ safe)\n# =====================================================\nIMAGE_TOKEN_ID = 32000  # llava placeholder\n\n@torch.no_grad()\ndef get_image_token_range_hf(llava_model, inputs):\n    \"\"\"\n    HF LLaVA uses a single <image> placeholder token in input_ids,\n    but internally merges vision patches into embeddings.\n\n    We compute img_end = img_start + num_patches.\n    \"\"\"\n    ids = inputs[\"input_ids\"][0]\n    pos = (ids == IMAGE_TOKEN_ID).nonzero(as_tuple=True)[0]\n    if len(pos) == 0:\n        return None\n\n    img_start = int(pos[0].item())\n\n    pv = inputs.get(\"pixel_values\", None)\n    if pv is None:\n        return None\n\n    # run vision tower to get actual number of patch tokens\n    vt = llava_model.vision_tower\n    pv = pv.to(next(vt.parameters()).device)\n\n    out = vt(pv, output_hidden_states=True)\n    # last_hidden_state: [B, patches, dim]\n    num_patches = int(out.last_hidden_state.shape[1])\n\n    img_end = img_start + num_patches\n    return img_start, img_end\n","metadata":{"_uuid":"669cf5aa-2dbb-4489-82a2-18f60eca0abe","_cell_guid":"6ffc3d45-5d56-446b-9321-61465cb348a1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-15T17:42:57.765217Z","iopub.execute_input":"2026-01-15T17:42:57.765831Z","iopub.status.idle":"2026-01-15T17:42:57.771181Z","shell.execute_reply.started":"2026-01-15T17:42:57.765800Z","shell.execute_reply":"2026-01-15T17:42:57.770571Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# =====================================================\n# ‚úÖ TEST 1 ‚Äî BASELINE INFERENCE (NO SPIN)\n# Same model, same processor, same prompt.\n# =====================================================\n\nfrom tqdm import tqdm\nimport json\nimport torch\n\n# -----------------------------\n# 0) Disable SPIN runtime flag\n# -----------------------------\ntry:\n    layers = get_llama_layers(model)\n    for layer in layers:\n        sa = layer.self_attn\n        if hasattr(sa, \"use_spin_img\"):\n            sa.use_spin_img = False\n    print(\"‚úÖ Disabled SPIN flags (use_spin_img=False) on all layers.\")\nexcept Exception as e:\n    print(\"‚ö†Ô∏è Could not disable SPIN flags. Error:\", e)\n\n# Reset debug counters\nspin_debug = {\n    \"forward_calls\": 0,\n    \"spin_active_calls\": 0,\n    \"q_len1_calls\": 0,\n    \"avg_suppressed_fraction_sum\": 0.0,\n}\n\n# -----------------------------\n# 1) Baseline generation loop\n# -----------------------------\nbaseline_results = []\n\nvision_calls = {\"count\": 0}\ndef vision_hook(module, inp, out):\n    vision_calls[\"count\"] += 1\n\ntry:\n    vision_handle = model.vision_tower.register_forward_hook(vision_hook)\nexcept Exception:\n    vision_handle = None\n\nprint(\"üöÄ Running BASELINE generation (NO SPIN)...\")\n\nN_SAMPLES = len(dataset)\nfor i, item in tqdm(list(enumerate(dataset[:N_SAMPLES])), total=N_SAMPLES):\n    q = item[\"question\"]\n    img = item[\"image\"].convert(\"RGB\")\n\n    # ‚úÖ SAME prompt as SPIN run\n    prompt = f\"USER: <image>\\n{q}\\nASSISTANT:\"\n\n    # ‚úÖ Correct processor call\n    inputs = processor(\n        text=prompt,\n        images=img,\n        return_tensors=\"pt\",\n        padding=True,\n    )\n\n    # ‚úÖ Your preferred device move code\n    for k, v in inputs.items():\n        if torch.is_tensor(v):\n            inputs[k] = v.to(\n                lm_device,\n                dtype=(torch.float16 if k == \"pixel_values\" else None)\n            )\n\n    # ‚úÖ Greedy decoding (same as SPIN run)\n    with torch.no_grad():\n        out_ids = model.generate(\n            **inputs,\n            max_new_tokens=128,\n            do_sample=False,\n            num_beams=1,\n            repetition_penalty=1.1,\n            pad_token_id=processor.tokenizer.eos_token_id,\n        )\n\n    # ‚úÖ Decode full text\n    decoded = processor.batch_decode(\n        out_ids,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=False\n    )[0]\n\n    # ‚úÖ Extract assistant answer robustly\n    if \"ASSISTANT:\" in decoded:\n        answer = decoded.split(\"ASSISTANT:\")[-1].strip()\n    elif \"### Assistant:\" in decoded:\n        answer = decoded.split(\"### Assistant:\")[-1].strip()\n    else:\n        answer = decoded.replace(prompt, \"\").strip()\n\n    baseline_results.append(\n        {\n            \"question_type\": item.get(\"question_type\", \"\"),\n            \"question_topic\": item.get(\"question_topic\", \"\"),\n            \"image_id\": item.get(\"image_id\", \"\"),\n            \"image_src\": item.get(\"image_src\", \"\"),\n            \"image_content\": item.get(\"image_content\", []),\n            \"question\": q,\n            \"gt_answer\": item.get(\"gt_answer\", \"\"),\n            \"model_answer\": answer,\n        }\n    )\n\nif vision_handle is not None:\n    vision_handle.remove()\n\nprint(\"‚úÖ Vision tower forward calls:\", vision_calls[\"count\"])\nprint(\"‚úÖ SPIN debug (should be 0 active calls):\", spin_debug)\n\n# -----------------------------\n# 2) Save baseline responses\n# -----------------------------\nBASELINE_RESPONSE_FILE = \"response_baseline_nospin.json\"\nwith open(BASELINE_RESPONSE_FILE, \"w\") as f:\n    json.dump(baseline_results, f, indent=2)\n\nprint(f\"‚úÖ Saved baseline responses to: {BASELINE_RESPONSE_FILE}\")\n\n# -----------------------------\n# 3) Quick sanity check prints\n# -----------------------------\nprint(\"\\nüîé Sample baseline outputs:\")\nfor j in range(min(3, len(baseline_results))):\n    print(\"=\" * 70)\n    print(\"Q:\", baseline_results[j][\"question\"])\n    print(\"A:\", baseline_results[j][\"model_answer\"][:400])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:13:20.325332Z","iopub.execute_input":"2026-01-15T18:13:20.325893Z","iopub.status.idle":"2026-01-15T18:17:33.037985Z","shell.execute_reply.started":"2026-01-15T18:13:20.325864Z","shell.execute_reply":"2026-01-15T18:17:33.037294Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Disabled SPIN flags (use_spin_img=False) on all layers.\nüöÄ Running BASELINE generation (NO SPIN)...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [04:12<00:00,  2.63s/it]","output_type":"stream"},{"name":"stdout","text":"‚úÖ Vision tower forward calls: 96\n‚úÖ SPIN debug (should be 0 active calls): {'forward_calls': 99808, 'spin_active_calls': 0, 'q_len1_calls': 0, 'avg_suppressed_fraction_sum': 0.0}\n‚úÖ Saved baseline responses to: response_baseline_nospin.json\n\nüîé Sample baseline outputs:\n======================================================================\nQ: What color is the fire hydrant cap in the picture?\nA: The fire hydrant cap in the picture is yellow.\n======================================================================\nQ: Who is sitting on the bench?\nA: No one is sitting on the bench.\n======================================================================\nQ: Which wooden stool has a vase with red flower on it?\nA: The left wooden stool has a vase with a red flower in it.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"\nTEMPLATE =  '''\nPlease act as an impartial and objective judge and evaluate the quality of the response provided by a Large Multimodal Model (LMM) to the user question. Your evaluation should be mainly based on whether the response is informative, and whether the response contains any hallucination. Hallucination, in this context, refers to a situation where the LMM generates a response that includes information not present or implied in the image or previous conversation. A hallucination could be a false claim about an object, action, emotion, or any other detail that is not grounded in the image.\n\nFor clarity, consider these examples:\n\n### Example 1:\n- Image Contents: Lynx, Cat\n- Question: How many cats can you see in this image?\n- Standard Human-Generated Answer: There are two cats visible in the image.\n- LMM Response to Evaluate: There is one cat in the image.\n- Analysis: The LMM's response is false because it claims there is only one cat in the image, when in fact there are two.\n- Hallucination: Yes.\n\n### Example 2:\n- Image Contents: Tree, Plant, Panda, Animal\n- Question: What can you see from this photo?\n- Standard Human-Generated Answer: In this photo we can see branches. We can see a panda on a branch. The background is blurry.\n- LMM Response to Evaluate: In the photo, I can see a large black and white panda sitting on a tree branch. The image captures the panda's unique appearance and its natural habitat, showcasing the beauty and rarity of these animals.\n- Analysis: The LMM's response is not false because it does not make any false claims about the image contents. For example, it does not claim that the panda is eating bamboo.\n- Hallucination: No.\n\n### Example 3:\n- Image Contents: Fashion accessory, Mammal, Dog, Umbrella\n- Question: Describe this image in details.\n- Standard Human-Generated Answer: In this picture I can see a dog laying under a yellow umbrella on a beach, and there is a rope on the right side. In the background I can see few persons talking or lying. At the top of the image, I can see the blue and clear sky.\n- LMM Response to Evaluate: The image features a brown dog laying on the sand at the beach, underneath a yellow umbrella. The dog is positioned near the water, and there are people in the background. The beach is sandy, and the sun is shining, creating a pleasant atmosphere. The dog is enjoying the shade provided by the umbrella, which is a common way to protect pets from the sun's harmful rays. The presence of people in the background suggests that the beach is a popular spot for leisure activities, such as sunbathing, swimming, and relaxing.\n- Analysis: The LMM's response is false because it claims that the dog is positioned near the water, but there is no water in the image.\n- Hallucination: Yes.\n\n### Example 4:\n- Image Contents: Clothing, Luggage and bags, Man, Building, Woman, Person\n- Question: Where is this photo taken?\n- Standard Human-Generated Answer: From the slogan \"Welcome to Houston, Texas\", we can infer that this photo is taken at an airport in Houston, Texas.\n- LMM Response to Evaluate: This photo is taken at an airport, specifically at the entrance of a terminal. However, the specific location of this airport cannot be determined from the image.\n- Analysis: Although the LMM's response does not explicitly mention the location \"Houston, Texas\" and does not perfectly answer the question, it makes no false claims about the image contents. Therefore, this response is not considered hallucination. It is OK if the LMM fails to directly answer the question, expresses uncertainty, or cannot include all the objects or details in the human-generated answer.\n- Hallucination: No.\n\n### Example 5:\n- Image Contents: Tree, Tower\n- Question: What environmental concerns are involved in this photo?\n- Standard Human-Generated Answer: This photo shows a smokestack emitting smoke. It may cause severe air pollution which threatens human health.\n- LMM Response to Evaluate: In the photo, there are several environmental concerns related to the smokestack emitting smoke. The smoke from the smokestack is a byproduct of industrial processes, which can contribute to air pollution, climate change, and negative impacts on human health. The smoke contains harmful pollutants, such as particulate matter, sulfur dioxide, nitrogen oxides, and carbon monoxide, which can lead to respiratory problems, heart disease, and other health issues. Additionally, the smoke contributes to the greenhouse effect, which can lead to global warming and climate change. The smokestack's emissions also affect the environment, as they can harm wildlife, vegetation, and ecosystems. Therefore, it is essential to address these environmental concerns by implementing measures to reduce emissions and promote sustainable practices in industrial processes.\n- Analysis: Although the LMM's response is significantly longer than the standard human-generated answer, it does not contain any false claims about the image contents. Instead, it provides additional general information about the environmental concerns, which can be inferred from the smoke emission. Such detailed analysis or reasoning should be considered as a positive aspect, as long as it contains no false claims.\n- Hallucination: No.\n\nWith these examples in mind, please help me evaluate whether the response by the LMM is informative, and whether hallucination exists in it, based on the comparison between the LMM's response and the factual information provided in the image contents, question, and the standard human-generated answer below.\n\nPlease note that the standard human-generated answer may only contain factual information but may not give a detailed analysis. Also, the standard human-generated answer may not be completely comprehensive in describing all the objects and their attributes, so please be a bit more cautious during evalutation. LMM's detailed analysis or reasoning should be encouraged.\n\nTo evaluate the LMM responses, first, begin your evaluation by providing a short explanation. Second, after providing your explanation, you must rate the response by choosing from the following options:\n- Rating: 6, very informative with good analysis or reasoning, no hallucination\n- Rating: 5, very informative, no hallucination\n- Rating: 4, somewhat informative, no hallucination\n- Rating: 3, not informative, no hallucination\n- Rating: 2, very informative, with hallucination\n- Rating: 1, somewhat informative, with hallucination\n- Rating: 0, not informative, with hallucination\n\n### Image Contents\n{}\n\n### Question\n{}\n\n### Standard Human-Generated Answer\n{}\n\n### LMM Response to Evaluate\n{}\n'''","metadata":{"_uuid":"c126a0f4-3efc-4d84-b98a-86940a52f994","_cell_guid":"2a06d371-2a7a-4735-a449-4d57b76b97ef","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-15T17:43:02.608921Z","iopub.execute_input":"2026-01-15T17:43:02.609248Z","iopub.status.idle":"2026-01-15T17:43:02.615317Z","shell.execute_reply.started":"2026-01-15T17:43:02.609219Z","shell.execute_reply":"2026-01-15T17:43:02.614673Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# =====================================================\n# 4) EVALUATION USING QWEN 2.5 7B (LOCAL JUDGE)  ‚úÖ NO BNB\n# =====================================================\n\n!pip install -q -U transformers accelerate\nprint(\"‚úÖ Eval libraries ready (NO bitsandbytes).\")\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport re\nimport json\nimport torch\nimport traceback\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nMODEL_ID = \"Qwen/Qwen2.5-3B-Instruct\"\n\nprint(f\"Loading judge model {MODEL_ID} in FP16 (no 4-bit)...\")\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\njudge = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\nprint(\"‚úÖ Judge loaded successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"OUTPUT_FILE = \"eval_result_without_SPIN.json\"\n\ndef get_local_rating(tokenizer, model, prompt):\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": (\n                \"You are an impartial AI Judge. Evaluate the response based on accuracy and hallucination. \"\n                \"Output the Explanation first, then the Rating.\"\n            ),\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=256,\n        do_sample=False,\n        temperature=0.0,\n    )\n\n    generated_ids = [\n        output_ids[len(input_ids):]\n        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return response\n\n\nwith open(BASELINE_RESPONSE_FILE, \"r\") as f:\n    records = json.load(f)\n\nprint(f\"Starting evaluation of {len(records)} records...\")\n\nevaluations = []\nfor i, record in enumerate(records):\n    image_content = \", \".join(record.get(\"image_content\", []))\n\n    input_text = TEMPLATE.format(\n        image_content,\n        record.get(\"question\", \"\"),\n        record.get(\"gt_answer\", \"\"),\n        record.get(\"model_answer\", \"\"),\n    )\n\n    try:\n        resp = get_local_rating(tokenizer, judge, input_text)\n\n        evaluations.append(\n            {\n                \"id\": i,\n                \"question_type\": record.get(\"question_type\", \"\"),\n                \"response\": resp,\n            }\n        )\n\n        snippet = resp.replace(\"\\n\", \" \")[:80]\n        print(f\"[{i+1}/{len(records)}] ‚úÖ {snippet}...\")\n\n    except Exception as e:\n        print(f\"‚ùå Error on {i}: {e}\")\n        if i == 0:\n            traceback.print_exc()\n\nwith open(OUTPUT_FILE, \"w\") as f:\n    json.dump(evaluations, f, indent=2)\n\nprint(\"üéâ Evaluation complete. Saved to\", OUTPUT_FILE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:18:00.924395Z","iopub.execute_input":"2026-01-15T18:18:00.925035Z","iopub.status.idle":"2026-01-15T18:25:52.962455Z","shell.execute_reply.started":"2026-01-15T18:18:00.925003Z","shell.execute_reply":"2026-01-15T18:25:52.961821Z"}},"outputs":[{"name":"stdout","text":"Starting evaluation of 96 records...\n[1/96] ‚úÖ Explanation: The LMM's response is accurate and does not contain any hallucinati...\n[2/96] ‚úÖ Explanation: The LMM's response is accurate and does not contain any hallucinati...\n[3/96] ‚úÖ Explanation: The LMM response is accurate and does not contain any hallucination...\n[4/96] ‚úÖ Explanation: The LMM response is not entirely accurate as it states there are tw...\n[5/96] ‚úÖ Explanation: The LMM response is not entirely accurate according to the image co...\n[6/96] ‚úÖ Explanation: The LMM response is informative as it correctly identifies the weat...\n[7/96] ‚úÖ Explanation: The LMM response is somewhat informative, but it contains a halluci...\n[8/96] ‚úÖ Explanation: The LMM response contains a hallucination as it provides a price of...\n[9/96] ‚úÖ Explanation: The LMM response is accurate and matches the standard human-generat...\n[10/96] ‚úÖ Explanation: The LMM's response contains a hallucination as it incorrectly state...\n[11/96] ‚úÖ Explanation: The LMM response is accurate and does not contain any hallucination...\n[12/96] ‚úÖ Explanation: The LMM response is not accurate as it states there are three bicyc...\n[13/96] ‚úÖ Explanation: The LMM response is somewhat informative but contains a hallucinati...\n[14/96] ‚úÖ Explanation: The LMM response is informative as it correctly identifies the weat...\n[15/96] ‚úÖ Explanation: The LMM response is somewhat informative but contains some hallucin...\n[16/96] ‚úÖ Explanation: The LMM response contains a hallucination. The image content clearl...\n[17/96] ‚úÖ Explanation: The LMM response is accurate and informative, matching the standard...\n[18/96] ‚úÖ Explanation: The LMM response contains a hallucination. The image does not conta...\n[19/96] ‚úÖ Explanation: The LMM response is not informative as it does not provide any rele...\n[20/96] ‚úÖ Explanation: The LMM response is accurate and does not contain any hallucination...\n[21/96] ‚úÖ Explanation: The LMM response contains a hallucination as it introduces new elem...\n[22/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[23/96] ‚úÖ Explanation: The LMM response is somewhat informative but contains hallucination...\n[24/96] ‚úÖ Explanation: The LMM response is accurate and does not contain any hallucination...\n[25/96] ‚úÖ Explanation: The LMM response is accurate and does not contain any hallucination...\n[26/96] ‚úÖ Explanation: The LMM response is not entirely accurate. The image indeed contain...\n[27/96] ‚úÖ Explanation: The LMM response is not entirely accurate. The standard human-gener...\n[28/96] ‚úÖ Explanation: The LMM response is not entirely accurate as it does not provide th...\n[29/96] ‚úÖ Explanation: The LMM response is informative as it correctly identifies the shee...\n[30/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[31/96] ‚úÖ Explanation: The LMM response is somewhat informative, but it contains a halluci...\n[32/96] ‚úÖ Explanation: The LMM's response is not accurate according to the image contents....\n[33/96] ‚úÖ Explanation: The LMM response is not accurate according to the standard human-ge...\n[34/96] ‚úÖ Explanation: The LMM's response contains a hallucination as it states that the j...\n[35/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[36/96] ‚úÖ Explanation: The LMM's response is informative as it correctly identifies the pr...\n[37/96] ‚úÖ Explanation: The LMM response contains a hallucination as it contradicts the ima...\n[38/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[39/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[40/96] ‚úÖ Explanation: The LMM's response is not entirely accurate as it does not provide ...\n[41/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[42/96] ‚úÖ Explanation: The LMM response is not entirely accurate based on the image conten...\n[43/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[44/96] ‚úÖ Explanation: The LMM's response is not accurate as there are actually three hors...\n[45/96] ‚úÖ Explanation: The LMM response is accurate according to the image contents. The s...\n[46/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[47/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[48/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[49/96] ‚úÖ Explanation: The LMM response is accurate and does not contain any hallucination...\n[50/96] ‚úÖ Explanation: The LMM response is not accurate according to the image contents an...\n[51/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[52/96] ‚úÖ Explanation: The LMM's response contains a hallucination as it states there are ...\n[53/96] ‚úÖ Explanation: The LMM response is not entirely accurate as it omits the coffee cu...\n[54/96] ‚úÖ Explanation: The LMM response is accurate and informative, matching the standard...\n[55/96] ‚úÖ Explanation: The LMM response is somewhat informative, but it introduces some el...\n[56/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[57/96] ‚úÖ Explanation: The LMM response is informative as it correctly identifies the umbr...\n[58/96] ‚úÖ Explanation: The LMM response is not entirely accurate based on the image conten...\n[59/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[60/96] ‚úÖ Explanation: The LMM's response contains a hallucination as it states there are ...\n[61/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[62/96] ‚úÖ Explanation: The LMM response is accurate and does not contain any hallucination...\n[63/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[64/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[65/96] ‚úÖ Explanation: The LMM response is accurate and does not contain any hallucination...\n[66/96] ‚úÖ Explanation: The LMM response is accurate and informative. It correctly identifi...\n[67/96] ‚úÖ Explanation: The LMM response is not entirely accurate as it mentions a cell pho...\n[68/96] ‚úÖ Explanation: The LMM's response is accurate and does not contain any hallucinati...\n[69/96] ‚úÖ Explanation: The LMM's response contains a hallucination. The image content clea...\n[70/96] ‚úÖ Explanation: The LMM response is not entirely accurate based on the image conten...\n[71/96] ‚úÖ Explanation: The LMM response is somewhat informative, as it describes the main ...\n[72/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[73/96] ‚úÖ Explanation: The LMM response contains hallucinations as it incorrectly states t...\n[74/96] ‚úÖ Explanation: The LMM's response is not accurate according to the image contents....\n[75/96] ‚úÖ Explanation: The LMM response is informative as it correctly identifies the pill...\n[76/96] ‚úÖ Explanation: The LMM response is accurate and does not contain any hallucination...\n[77/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[78/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[79/96] ‚úÖ Explanation: The LMM response is somewhat informative, as it describes the main ...\n[80/96] ‚úÖ Explanation: The LMM response contradicts the standard human-generated answer. T...\n[81/96] ‚úÖ Explanation: The LMM response contains a hallucination as it incorrectly states ...\n[82/96] ‚úÖ Explanation: The LMM's response is accurate and does not contain any hallucinati...\n[83/96] ‚úÖ Explanation: The LMM response is not entirely accurate based on the image conten...\n[84/96] ‚úÖ Explanation: The LMM's response is accurate and does not contain any hallucinati...\n[85/96] ‚úÖ Explanation: The LMM response is not entirely accurate. While it correctly ident...\n[86/96] ‚úÖ Explanation: The LMM response is informative as it provides a plausible interpre...\n[87/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[88/96] ‚úÖ Explanation: The LMM response is not entirely accurate as it contains a grammati...\n[89/96] ‚úÖ Explanation: The LMM's response contains a hallucination as it incorrectly state...\n[90/96] ‚úÖ Explanation: The LMM response contains a hallucination as it introduces new info...\n[91/96] ‚úÖ Explanation: The LMM response is not informative as it does not provide any rele...\n[92/96] ‚úÖ Explanation: The LMM response contains a hallucination as it incorrectly identif...\n[93/96] ‚úÖ Explanation: The LMM's response contains a hallucination as it contradicts the i...\n[94/96] ‚úÖ Explanation: The LMM response is accurate and does not contain any false claims ...\n[95/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[96/96] ‚úÖ Explanation: The LMM response contains a significant hallucination. The image do...\nüéâ Evaluation complete. Saved to eval_result_without_SPIN.json\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# =====================================================\n#  DOWNLOAD OUTPUTS (COLAB)\n# =====================================================\nfrom google.colab import files\n\nfiles.download(BASELINE_RESPONSE_FILE)\nfiles.download(OUTPUT_FILE)\n\nprint(\"‚úÖ Downloads triggered.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =====================================================\n# 5) SCORE PARSING + STATS\n# =====================================================\n\ndef parse_rating(text: str) -> int:\n    t = (text or \"\").lower()\n    m = re.search(r\"rating[:\\s\\*\\-]*([0-6])\", t)\n    if not m:\n        return 0\n    return int(m.group(1))\n\n\nscores = [parse_rating(x.get(\"response\", \"\")) for x in evaluations]\n\nif len(scores) == 0:\n    print(\"‚ö†Ô∏è No scores found\")\nelse:\n    avg = sum(scores) / len(scores)\n    hallucination_count = sum(1 for s in scores if s < 3)\n    hal_rate = hallucination_count / len(scores)\n\n    print(\"=\" * 40)\n    print(f\"Total samples: {len(scores)}\")\n    print(f\"Average score: {avg:.2f} (Higher is better)\")\n    print(f\"Hallucination rate: {hal_rate:.2f} (Lower is better)\")\n    print(\"=\" * 40)\n\n\nQUESTION_TYPE_NAMES = [\n    \"holistic\",\n    \"counting\",\n    \"relation\",\n    \"environment\",\n    \"other\",\n    \"attribute\",\n    \"adversarial\",\n    \"comparison\",\n]\n\nscores_each = {k: [] for k in QUESTION_TYPE_NAMES}\n\nfor ev in evaluations:\n    qtype = (ev.get(\"question_type\") or \"other\").lower()\n    if qtype not in scores_each:\n        qtype = \"other\"\n    scores_each[qtype].append(parse_rating(ev.get(\"response\", \"\")))\n\nprint(\"\\nAverage score by question type:\")\nprint(\"-\" * 45)\nfor k in QUESTION_TYPE_NAMES:\n    if scores_each[k]:\n        print(f\"{k:<15}: {sum(scores_each[k])/len(scores_each[k]):.2f}\")\n    else:\n        print(f\"{k:<15}: N/A\")\n\nprint(\"-\" * 45)\nprint(f\"{'overall':<15}: {avg:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T18:27:03.013710Z","iopub.execute_input":"2026-01-15T18:27:03.014010Z","iopub.status.idle":"2026-01-15T18:27:03.023319Z","shell.execute_reply.started":"2026-01-15T18:27:03.013982Z","shell.execute_reply":"2026-01-15T18:27:03.022565Z"}},"outputs":[{"name":"stdout","text":"========================================\nTotal samples: 96\nAverage score: 2.86 (Higher is better)\nHallucination rate: 0.43 (Lower is better)\n========================================\n\nAverage score by question type:\n---------------------------------------------\nholistic       : 3.83\ncounting       : 2.67\nrelation       : 2.17\nenvironment    : 4.08\nother          : 2.67\nattribute      : 3.17\nadversarial    : 1.42\ncomparison     : 2.92\n---------------------------------------------\noverall        : 2.86\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# =====================================================\n# 6) RUN INFERENCE WITH SPIN + SAVE RESPONSES  (UPDATED ‚úÖ)\n# =====================================================\nfrom tqdm import tqdm\nimport json\nimport torch\n\n# ‚úÖ Fix processor config warning for v4.46+\nif not hasattr(processor, \"patch_size\") or processor.patch_size is None:\n    processor.patch_size = model.config.vision_config.patch_size\n\nif not hasattr(processor, \"vision_feature_select_strategy\") or processor.vision_feature_select_strategy is None:\n    processor.vision_feature_select_strategy = \"default\"\n\n\nstart_layer = 0\nlayers = get_llama_layers(model)\nend_layer = len(layers)\nkeep_head_ratio = 0.95\nsuppression_alpha = 0.08\n\nresults = []\n\n# Debug: hook the vision tower to ensure images are used\nvision_calls = {\"count\": 0}\ndef vision_hook(module, inp, out):\n    vision_calls[\"count\"] += 1\n\ntry:\n    vision_handle = model.vision_tower.register_forward_hook(vision_hook)\nexcept Exception:\n    vision_handle = None\n\nprint(f\"Running generation for {len(dataset)} samples...\")\n\nN_SAMPLES = len(dataset)\nfor i, item in tqdm(list(enumerate(dataset[:N_SAMPLES])), total=N_SAMPLES):\n    q = item[\"question\"]\n    img = item[\"image\"].convert(\"RGB\")\n\n    prompt = f\"USER: <image>\\n{q}\\nASSISTANT:\"\n\n    inputs = processor(\n        text=prompt,\n        images=img,\n        return_tensors=\"pt\",\n        padding=True,\n    )\n\n    # if i == 0:\n    #     ids = inputs[\"input_ids\"][0].tolist()\n    \n    #     print(\"\\n================== FULL TOKEN DUMP ==================\")\n    #     print(\"Total tokens:\", len(ids))\n    \n    #     for idx, tok in enumerate(ids):\n    #         token_str = processor.tokenizer.decode([tok], skip_special_tokens=False)\n    #         token_str = token_str.replace(\"\\n\", \"\\\\n\")  # make newlines visible\n    #         print(f\"{idx:04d} | {tok:6d} | {repr(token_str)}\")\n    #     print(\"=====================================================\\n\")\n\n\n    # ‚úÖ Move only input_ids/attention_mask to LM device\n    for k in [\"input_ids\", \"attention_mask\"]:\n        if k in inputs:\n            inputs[k] = inputs[k].to(lm_device)\n\n    # ‚úÖ Apply SPIN once (first sample)\n    if i == 0:\n        rng = get_image_token_range_hf(model, inputs)\n        if rng is None:\n            print(\"‚ö†Ô∏è Could not find correct image token range. SPIN disabled.\")\n            img_start_idx, img_end_idx = 0, 0\n            use_spin = False\n        else:\n            img_start_idx, img_end_idx = rng\n            use_spin = True\n\n        print(\"‚úÖ Image span:\", (img_start_idx, img_end_idx))\n\n        if use_spin:\n            apply_spin_to_llava(\n                model,\n                start_layer=start_layer,\n                end_layer=end_layer,\n                img_start_idx=img_start_idx,\n                img_end_idx=img_end_idx,\n                keep_head_ratio=keep_head_ratio,\n                suppression_alpha=suppression_alpha,\n                use_spin_img=True,\n            )\n\n        pv = inputs.get(\"pixel_values\", None)\n        if pv is not None:\n            print(\"‚úÖ pixel_values:\", tuple(pv.shape), pv.dtype, pv.device)\n\n    # ‚úÖ GREEDY decoding\n    with torch.no_grad():\n        out_ids = model.generate(\n            **inputs,\n            max_new_tokens=128,\n            do_sample=False,\n            num_beams=1,\n            repetition_penalty=1.1,\n            pad_token_id=processor.tokenizer.eos_token_id,\n        )\n\n    decoded = processor.batch_decode(\n        out_ids,\n        skip_special_tokens=True,\n        clean_up_tokenization_spaces=False\n    )[0]\n\n    # ‚úÖ Robust answer extraction\n    if \"ASSISTANT:\" in decoded:\n        answer = decoded.split(\"ASSISTANT:\")[-1].strip()\n    elif \"### Assistant:\" in decoded:\n        answer = decoded.split(\"### Assistant:\")[-1].strip()\n    else:\n        answer = decoded.replace(prompt, \"\").strip()\n\n    results.append(\n        {\n            \"question_type\": item.get(\"question_type\", \"\"),\n            \"question_topic\": item.get(\"question_topic\", \"\"),\n            \"image_id\": item.get(\"image_id\", \"\"),\n            \"image_src\": item.get(\"image_src\", \"\"),\n            \"image_content\": item.get(\"image_content\", []),\n            \"question\": q,\n            \"gt_answer\": item.get(\"gt_answer\", \"\"),\n            \"model_answer\": answer,\n        }\n    )\n\nif vision_handle is not None:\n    vision_handle.remove()\n\nprint(\"‚úÖ Vision tower forward calls:\", vision_calls[\"count\"])\nprint(\"‚úÖ SPIN debug:\", spin_debug)\n\nif spin_debug.get(\"spin_active_calls\", 0) > 0:\n    avg_supp = spin_debug[\"avg_suppressed_fraction_sum\"] / spin_debug[\"spin_active_calls\"]\n    print(f\"‚úÖ Avg suppressed head fraction (decode): {avg_supp:.3f}\")\n\nRESPONSE_FILE = \"response_mymodel.json\"\nwith open(RESPONSE_FILE, \"w\") as f:\n    json.dump(results, f, indent=2)\n\nprint(f\"‚úÖ Saved responses to {RESPONSE_FILE}\")\n\n# -----------------------------\n# 3) Quick sanity check prints\n# -----------------------------\nprint(\"\\nüîé Sample SPIN outputs:\")\nfor j in range(min(3, len(results))):\n    print(\"=\" * 70)\n    print(\"Q:\", results[j][\"question\"])\n    print(\"A:\", results[j][\"model_answer\"][:400])","metadata":{"_uuid":"bf7c40b1-67c0-45cb-917f-38957d17dcf8","_cell_guid":"4032f50d-b6a1-454f-be45-b1782800b774","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-15T17:45:15.173043Z","iopub.execute_input":"2026-01-15T17:45:15.173806Z","iopub.status.idle":"2026-01-15T17:50:00.385023Z","shell.execute_reply.started":"2026-01-15T17:45:15.173766Z","shell.execute_reply":"2026-01-15T17:50:00.384345Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Running generation for 96 samples...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/96 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"‚úÖ Image span: (5, 582)\n‚úÖ SPIN patched on layers [0, 32). keep=0.95, alpha=0.08\n‚úÖ pixel_values: (1, 3, 336, 336) torch.float32 cpu\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 96/96 [04:45<00:00,  2.97s/it]","output_type":"stream"},{"name":"stdout","text":"‚úÖ Vision tower forward calls: 97\n‚úÖ SPIN debug: {'forward_calls': 102656, 'spin_active_calls': 99424, 'q_len1_calls': 99424, 'avg_suppressed_fraction_sum': 6214.0}\n‚úÖ Avg suppressed head fraction (decode): 0.062\n‚úÖ Saved responses to response_mymodel.json\n\nüîé Sample SPIN outputs:\n======================================================================\nQ: What color is the fire hydrant cap in the picture?\nA: The fire hydrant cap in the image is yellow.\n======================================================================\nQ: Who is sitting on the bench?\nA: No one is sitting on the bench.\n======================================================================\nQ: Which wooden stool has a vase with red flower on it?\nA: The left wooden stool has the vase with the red flower.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(spin_debug)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T17:51:25.484615Z","iopub.execute_input":"2026-01-15T17:51:25.485246Z","iopub.status.idle":"2026-01-15T17:51:25.489454Z","shell.execute_reply.started":"2026-01-15T17:51:25.485212Z","shell.execute_reply":"2026-01-15T17:51:25.488736Z"}},"outputs":[{"name":"stdout","text":"{'forward_calls': 102656, 'spin_active_calls': 99424, 'q_len1_calls': 99424, 'avg_suppressed_fraction_sum': 6214.0}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"OUTPUT_FILE_SPIN = \"eval_result.json\"\ndef get_local_rating(tokenizer, model, prompt):\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": (\n                \"You are an impartial AI Judge. Evaluate the response based on accuracy and hallucination. \"\n                \"Output the Explanation first, then the Rating.\"\n            ),\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True,\n    )\n\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=256,\n        do_sample=False,\n        temperature=0.0,\n    )\n\n    generated_ids = [\n        output_ids[len(input_ids):]\n        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return response\n\n\nwith open(RESPONSE_FILE, \"r\") as f:\n    records = json.load(f)\n\nprint(f\"Starting evaluation of {len(records)} records...\")\n\nevaluations = []\nfor i, record in enumerate(records):\n    image_content = \", \".join(record.get(\"image_content\", []))\n\n    input_text = TEMPLATE.format(\n        image_content,\n        record.get(\"question\", \"\"),\n        record.get(\"gt_answer\", \"\"),\n        record.get(\"model_answer\", \"\"),\n    )\n\n    try:\n        resp = get_local_rating(tokenizer, judge, input_text)\n\n        evaluations.append(\n            {\n                \"id\": i,\n                \"question_type\": record.get(\"question_type\", \"\"),\n                \"response\": resp,\n            }\n        )\n\n        snippet = resp.replace(\"\\n\", \" \")[:80]\n        print(f\"[{i+1}/{len(records)}] ‚úÖ {snippet}...\")\n\n    except Exception as e:\n        print(f\"‚ùå Error on {i}: {e}\")\n        if i == 0:\n            traceback.print_exc()\n\nwith open(OUTPUT_FILE_SPIN, \"w\") as f:\n    json.dump(evaluations, f, indent=2)\n\nprint(\"üéâ Evaluation complete. Saved to\", OUTPUT_FILE_SPIN)\n","metadata":{"_uuid":"18278c19-4874-4e73-af12-136846fc08cf","_cell_guid":"9029f0e4-1cdb-40d5-b6ca-ef9efb4f3ba7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-15T17:56:32.719842Z","iopub.execute_input":"2026-01-15T17:56:32.720797Z","iopub.status.idle":"2026-01-15T18:05:28.703172Z","shell.execute_reply.started":"2026-01-15T17:56:32.720750Z","shell.execute_reply":"2026-01-15T18:05:28.702431Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Eval libraries ready (NO bitsandbytes).\nLoading judge model Qwen/Qwen2.5-3B-Instruct in FP16 (no 4-bit)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c1ca83edb1f44dbb35446401738cd8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a5563c85bda4c7b8195e8fd38779f69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7511393ffdce4d12b9cf407ed9cc838b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63d4657b2d8247fca26ebd674f775294"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eee0dd6b04dd4f9c9bbf24bea081f85f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9be666398854a2b80d2acfe1c960e0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08c61cf07d9b4660847d2feb5c3138ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96c04d59f7fc448781cfdedcc2d239a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95c3069085794566b3b2c9bdf751f5d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9db7755c98c64950b107956eea9861cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cceb12c98d941e5933bfeb4e3d1de6a"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Judge loaded successfully!\nStarting evaluation of 96 records...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  single_beam_wrong_parameter_msg = (\n/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  minor_issues[\"early_stopping\"] = single_beam_wrong_parameter_msg.format(\n/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:623: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n  raise ValueError(\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"[1/96] ‚úÖ Explanation: The LMM's response is accurate and does not contain any hallucinati...\n[2/96] ‚úÖ Explanation: The LMM's response is accurate and does not contain any hallucinati...\n[3/96] ‚úÖ Explanation: The LMM response is accurate and does not contain any hallucination...\n[4/96] ‚úÖ Explanation: The LMM's response is not accurate according to the image contents....\n[5/96] ‚úÖ Explanation: The LMM response is not entirely accurate according to the image co...\n[6/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[7/96] ‚úÖ Explanation: The LMM response is somewhat informative but contains hallucination...\n[8/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[9/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[10/96] ‚úÖ Explanation: The LMM's response contains a hallucination as it incorrectly state...\n[11/96] ‚úÖ Explanation: The LMM's response is accurate and does not contain any hallucinati...\n[12/96] ‚úÖ Explanation: The LMM response is not accurate as it states there are three bicyc...\n[13/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[14/96] ‚úÖ Explanation: The LMM response is informative as it correctly identifies the pres...\n[15/96] ‚úÖ Explanation: The LMM response is somewhat informative but contains some hallucin...\n[16/96] ‚úÖ Explanation: The LMM response contains a hallucination. The image does not provi...\n[17/96] ‚úÖ Explanation: The LMM response is not entirely accurate as it mentions a green ha...\n[18/96] ‚úÖ Explanation: The LMM response contains a hallucination. The image does not conta...\n[19/96] ‚úÖ Explanation: The LMM response is not informative as it does not provide any rele...\n[20/96] ‚úÖ Explanation: The LMM response is accurate and does not contain any hallucination...\n[21/96] ‚úÖ Explanation: The LMM response is not entirely accurate as it incorrectly identif...\n[22/96] ‚úÖ Explanation: The LMM response is somewhat informative, as it correctly identifie...\n[23/96] ‚úÖ Explanation: The LMM's response is informative and does not contain any hallucin...\n[24/96] ‚úÖ Explanation: The LMM response is informative as it correctly identifies the numb...\n[25/96] ‚úÖ Explanation: The LMM response is not entirely accurate as it only mentions one d...\n[26/96] ‚úÖ Explanation: The LMM's response is not informative as it does not match the imag...\n[27/96] ‚úÖ Explanation: The LMM response is not entirely accurate. While it correctly ident...\n[28/96] ‚úÖ Explanation: The LMM response is informative as it correctly identifies the pres...\n[29/96] ‚úÖ Explanation: The LMM response contains a hallucination as it introduces new elem...\n[30/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[31/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[32/96] ‚úÖ Explanation: The LMM's response is not accurate according to the image contents....\n[33/96] ‚úÖ Explanation: The LMM response contains a hallucination as it contradicts the sta...\n[34/96] ‚úÖ Explanation: The LMM response contains a hallucination as it states that there i...\n[35/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[36/96] ‚úÖ Explanation: The LMM response is informative as it correctly identifies the pres...\n[37/96] ‚úÖ Explanation: The LMM response is not entirely accurate. The standard human-gener...\n[38/96] ‚úÖ Explanation: The LMM response is somewhat informative but contains a hallucinati...\n[39/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[40/96] ‚úÖ Explanation: The LMM response does not contain any false claims about the image ...\n[41/96] ‚úÖ Explanation: The LMM response contains hallucinations as it does not match the c...\n[42/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[43/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[44/96] ‚úÖ Explanation: The LMM's response is not accurate as there are actually three hors...\n[45/96] ‚úÖ Explanation: The LMM response is accurate according to the image contents. The s...\n[46/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[47/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[48/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[49/96] ‚úÖ Explanation: The LMM response is accurate and does not contain any hallucination...\n[50/96] ‚úÖ Explanation: The LMM response is informative as it correctly identifies the pres...\n[51/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[52/96] ‚úÖ Explanation: The LMM's response contains a hallucination as it states there are ...\n[53/96] ‚úÖ Explanation: The LMM response is not entirely accurate as it omits the coffee cu...\n[54/96] ‚úÖ Explanation: The LMM response contains a significant hallucination. The image cl...\n[55/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[56/96] ‚úÖ Explanation: The LMM response is somewhat informative, but it contains a halluci...\n[57/96] ‚úÖ Explanation: The LMM response is not entirely accurate as it mentions the umbrel...\n[58/96] ‚úÖ Explanation: The LMM response contains a significant amount of information not p...\n[59/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[60/96] ‚úÖ Explanation: The LMM's response contains a hallucination as it states there are ...\n[61/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[62/96] ‚úÖ Explanation: The LMM response is not accurate according to the image contents. T...\n[63/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[64/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[65/96] ‚úÖ Explanation: The LMM response is accurate and does not contain any hallucination...\n[66/96] ‚úÖ Explanation: The LMM response is not informative as it contradicts the image con...\n[67/96] ‚úÖ Explanation: The LMM response is not entirely accurate. While it correctly ident...\n[68/96] ‚úÖ Explanation: The LMM's response is not entirely accurate as it incorrectly ident...\n[69/96] ‚úÖ Explanation: The LMM's response contains a hallucination as it states that the m...\n[70/96] ‚úÖ Explanation: The LMM's response contains a significant hallucination. The image ...\n[71/96] ‚úÖ Explanation: The LMM response is somewhat informative, but it introduces new ele...\n[72/96] ‚úÖ Explanation: The LMM response is informative as it correctly identifies the keyb...\n[73/96] ‚úÖ Explanation: The LMM response contains hallucinations as it mentions colors (lig...\n[74/96] ‚úÖ Explanation: The LMM's response is not accurate according to the image contents....\n[75/96] ‚úÖ Explanation: The LMM response is not entirely accurate as it only mentions the c...\n[76/96] ‚úÖ Explanation: The LMM response is not entirely accurate as it incorrectly identif...\n[77/96] ‚úÖ Explanation: The LMM response is not entirely accurate. While it correctly ident...\n[78/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[79/96] ‚úÖ Explanation: The LMM response is somewhat informative, as it describes the main ...\n[80/96] ‚úÖ Explanation: The LMM response is not informative as it does not provide any rele...\n[81/96] ‚úÖ Explanation: The LMM response contains a hallucination as it incorrectly states ...\n[82/96] ‚úÖ Explanation: The LMM's response is accurate and does not contain any hallucinati...\n[83/96] ‚úÖ Explanation: The LMM response is not accurate according to the image contents. T...\n[84/96] ‚úÖ Explanation: The LMM response is accurate and does not contain any hallucination...\n[85/96] ‚úÖ Explanation: The LMM response is not entirely accurate. While it correctly ident...\n[86/96] ‚úÖ Explanation: The LMM response does not provide any information about the weather...\n[87/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[88/96] ‚úÖ Explanation: The LMM response is not entirely accurate as it omits the article \"...\n[89/96] ‚úÖ Explanation: The LMM response is not informative as it does not provide the colo...\n[90/96] ‚úÖ Explanation: The LMM response contains a hallucination as it introduces an objec...\n[91/96] ‚úÖ Explanation: The LMM response is not informative as it does not provide any rele...\n[92/96] ‚úÖ Explanation: The LMM response contains a hallucination as it incorrectly identif...\n[93/96] ‚úÖ Explanation: The LMM response contains a hallucination as it introduces elements...\n[94/96] ‚úÖ Explanation: The LMM response is informative as it correctly identifies the time...\n[95/96] ‚úÖ Explanation: The LMM response is informative and does not contain any hallucinat...\n[96/96] ‚úÖ Explanation: The LMM response contains a hallucination. The image does not show ...\nüéâ Evaluation complete. Saved to eval_result.json\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# =====================================================\n# 7) SCORE PARSING + STATS\n# =====================================================\n\ndef parse_rating(text: str) -> int:\n    t = (text or \"\").lower()\n    m = re.search(r\"rating[:\\s\\*\\-]*([0-6])\", t)\n    if not m:\n        return 0\n    return int(m.group(1))\n\n\nscores = [parse_rating(x.get(\"response\", \"\")) for x in evaluations]\n\nif len(scores) == 0:\n    print(\"‚ö†Ô∏è No scores found\")\nelse:\n    avg = sum(scores) / len(scores)\n    hallucination_count = sum(1 for s in scores if s < 3)\n    hal_rate = hallucination_count / len(scores)\n\n    print(\"=\" * 40)\n    print(f\"Total samples: {len(scores)}\")\n    print(f\"Average score: {avg:.2f} (Higher is better)\")\n    print(f\"Hallucination rate: {hal_rate:.2f} (Lower is better)\")\n    print(\"=\" * 40)\n\n\nQUESTION_TYPE_NAMES = [\n    \"holistic\",\n    \"counting\",\n    \"relation\",\n    \"environment\",\n    \"other\",\n    \"attribute\",\n    \"adversarial\",\n    \"comparison\",\n]\n\nscores_each = {k: [] for k in QUESTION_TYPE_NAMES}\n\nfor ev in evaluations:\n    qtype = (ev.get(\"question_type\") or \"other\").lower()\n    if qtype not in scores_each:\n        qtype = \"other\"\n    scores_each[qtype].append(parse_rating(ev.get(\"response\", \"\")))\n\nprint(\"\\nAverage score by question type:\")\nprint(\"-\" * 45)\nfor k in QUESTION_TYPE_NAMES:\n    if scores_each[k]:\n        print(f\"{k:<15}: {sum(scores_each[k])/len(scores_each[k]):.2f}\")\n    else:\n        print(f\"{k:<15}: N/A\")\n\nprint(\"-\" * 45)\nprint(f\"{'overall':<15}: {avg:.2f}\")","metadata":{"_uuid":"039e31ef-7a8a-432c-8132-79491467a7f9","_cell_guid":"93a586ba-a06c-41a6-a1bd-99879887866f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-15T18:05:46.694605Z","iopub.execute_input":"2026-01-15T18:05:46.695294Z","iopub.status.idle":"2026-01-15T18:05:46.704717Z","shell.execute_reply.started":"2026-01-15T18:05:46.695260Z","shell.execute_reply":"2026-01-15T18:05:46.704164Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"========================================\nTotal samples: 96\nAverage score: 2.45 (Higher is better)\nHallucination rate: 0.49 (Lower is better)\n========================================\n\nAverage score by question type:\n---------------------------------------------\nholistic       : 4.08\ncounting       : 2.08\nrelation       : 2.17\nenvironment    : 2.67\nother          : 2.08\nattribute      : 2.08\nadversarial    : 1.50\ncomparison     : 2.92\n---------------------------------------------\noverall        : 2.45\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"\n# =====================================================\n#  DOWNLOAD OUTPUTS (COLAB)\n# =====================================================\nfrom google.colab import files\n\nfiles.download(RESPONSE_FILE)\nfiles.download(OUTPUT_FILE_SPIN)\n\nprint(\"‚úÖ Downloads triggered.\")","metadata":{"_uuid":"5544eefb-10ba-4610-a23f-98c03717cf8e","_cell_guid":"ceee7b47-99f2-4b4c-b411-afee469798ea","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-15T18:06:10.899557Z","iopub.execute_input":"2026-01-15T18:06:10.900124Z","iopub.status.idle":"2026-01-15T18:06:10.998423Z","shell.execute_reply.started":"2026-01-15T18:06:10.900095Z","shell.execute_reply":"2026-01-15T18:06:10.997882Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"download(\"download_eded2362-1e40-4c60-81a1-991a0b8c590f\", \"response_mymodel.json\", 63372)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"download(\"download_461b559a-7de8-4b01-aa0c-11f17864f7ff\", \"eval_result.json\", 51285)"},"metadata":{}},{"name":"stdout","text":"‚úÖ Downloads triggered.\n","output_type":"stream"}],"execution_count":17}]}